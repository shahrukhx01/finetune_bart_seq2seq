{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A Novel Approach for text-to-SQL: Dual Transformers Approach.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rpU5YY-P9Esp",
        "outputId": "5848a507-45ad-4e10-cac6-d022f4329944"
      },
      "source": [
        "pip install transformers"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.11.2)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.46)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (5.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.0.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.17)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.17->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bbegy0x7W6jG"
      },
      "source": [
        "from transformers import BartTokenizer, BartForConditionalGeneration, BartConfig,AutoModel, AutoTokenizer\n",
        "from torch import Tensor, device\n",
        "import uuid\n",
        "import pandas as pd\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nRvVjotz9Gsi",
        "outputId": "4fc39425-c893-4b40-b741-418df608543b"
      },
      "source": [
        "torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "## define the model hub's model name\n",
        "model_name = \"shahrukhx01/schema-aware-denoising-bart-large-cnn-text2sql\"\n",
        "\n",
        "## load model and tokenizer\n",
        "model = BartForConditionalGeneration.from_pretrained('shahrukhx01/schema-aware-denoising-bart-large-cnn-text2sql').to(torch_device)\n",
        "tokenizer = BartTokenizer.from_pretrained('shahrukhx01/schema-aware-denoising-bart-large-cnn-text2sql')\n",
        "\n",
        "# prepare question, this is how the table header looks like for this example\n",
        "##['Player', 'No.', 'Nationality', 'Position', 'Years in Toronto', 'School/Club Team']\n",
        "\n",
        "## we have to encode schema and concat the question alonside it as follows\n",
        "question_schema = \"\"\"What is terrence's  nationality? \n",
        "                      </s> <col0> Player : text <col1> No. : text <col2> Nationality : text \n",
        "                      <col3> Position : text <col4> Years in Toronto : text <col5>  School/Club Team : text\"\"\"\n",
        "\n",
        "## tokenize question_schema\n",
        "inputs = tokenizer([question_schema], max_length=1024, return_tensors='pt').to(torch_device)\n",
        "\n",
        "# generate SQL\n",
        "text_query_ids = model.generate(inputs['input_ids'], num_beams=4, min_length=0, max_length=125, early_stopping=True)\n",
        "prediction = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in text_query_ids][0]\n",
        "\n",
        "##magic!\n",
        "print(prediction)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SELECT `  <col2>  ` FROM ` table ` WHERE `  <col0>  ` = ` Terrence `\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ihD5PRRdSove",
        "outputId": "87c02b89-4dc5-48b0-8b1f-df0da9a04bff"
      },
      "source": [
        "\n",
        "## encoding model and tokenizer\n",
        "encoding_model_name = 'shahrukhx01/paraphrase-mpnet-base-v2-fuzzy-matcher'\n",
        "encoding_model = AutoModel.from_pretrained(encoding_model_name).to(torch_device)\n",
        "encoding_tokenizer = AutoTokenizer.from_pretrained(encoding_model_name)\n",
        "\n",
        "## define data, we will define rows and header and column types of each column separately here\n",
        "rows = [['Aleksandar RadojeviÄ‡', '25', 'Serbia', 'Center', '1999-2000', 'Barton CC (KS)'], ['Shawn Respert', '31', 'United States', 'Guard', '1997-98', 'Michigan State'], ['Quentin Richardson', 'N/A', 'United States', 'Forward', '2013-present', 'DePaul'], ['Alvin Robertson', '7, 21', 'United States', 'Guard', '1995-96', 'Arkansas'], ['Carlos Rogers', '33, 34', 'United States', 'Forward-Center', '1995-98', 'Tennessee State'], ['Roy Rogers', '9', 'United States', 'Forward', '1998', 'Alabama'], ['Jalen Rose', '5', 'United States', 'Guard-Forward', '2003-06', 'Michigan'], ['Terrence Ross', '31', 'United States', 'Guard', '2012-present', 'Washington']]\n",
        "header = ['Player', 'No.', 'Nationality', 'Position', 'Years in Toronto', 'School/Club Team']\n",
        "header_column_types = ['text', 'text', 'text', 'text', 'text', 'text']\n",
        "\n",
        "## define the helper functions below\n",
        "def cos_sim(a: Tensor, b: Tensor):\n",
        "    \"\"\"\n",
        "    borrowed from sentence transformers repo\n",
        "    Computes the cosine similarity cos_sim(a[i], b[j]) for all i and j.\n",
        "    :return: Matrix with res[i][j]  = cos_sim(a[i], b[j])\n",
        "    \"\"\"\n",
        "    if not isinstance(a, torch.Tensor):\n",
        "        a = torch.tensor(a)\n",
        "\n",
        "    if not isinstance(b, torch.Tensor):\n",
        "        b = torch.tensor(b)\n",
        "\n",
        "    if len(a.shape) == 1:\n",
        "        a = a.unsqueeze(0)\n",
        "\n",
        "    if len(b.shape) == 1:\n",
        "        b = b.unsqueeze(0)\n",
        "\n",
        "    a_norm = torch.nn.functional.normalize(a, p=2, dim=1)\n",
        "    b_norm = torch.nn.functional.normalize(b, p=2, dim=1)\n",
        "    return torch.mm(a_norm, b_norm.transpose(0, 1))\n",
        "\n",
        "\n",
        "#Mean Pooling - Take attention mask into account for correct averaging\n",
        "def mean_pooling(model_output, attention_mask):\n",
        "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
        "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
        "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
        "\n",
        "## get embedding of a word\n",
        "def get_embedding(value):\n",
        "    value = value.lower()\n",
        "    value = [\" \".join([x for x in value])]\n",
        "    # Tokenize sentences\n",
        "    encoded_input = encoding_tokenizer(value, padding=True, truncation=True, return_tensors='pt').to(torch_device)\n",
        "\n",
        "    # Compute token embeddings\n",
        "    with torch.no_grad():\n",
        "        model_output = encoding_model(**encoded_input)\n",
        "\n",
        "    # Perform pooling. In this case, max pooling.\n",
        "    embedding = mean_pooling(model_output, encoded_input['attention_mask'])\n",
        "    return embedding\n",
        "\n",
        "## encodes categorical data into vector space\n",
        "def encode_data(data, header, header_types):\n",
        "    table = pd.DataFrame(data, columns=header)\n",
        "    data = {}\n",
        "    #cell = \" \".join([x for x in generated_data])\n",
        "    for header_val, header_type_val in zip(header, header_types):\n",
        "        encoded_vals = Tensor().to(torch_device)\n",
        "        if header_type_val == 'text':\n",
        "            for value in table[header_val]:\n",
        "                #encoded_vals.append()\n",
        "                encoded_vals  = torch.cat((encoded_vals, get_embedding(value)), 0)\n",
        "        data[header_val] = encoded_vals.cpu()\n",
        "    return data, table\n",
        "\n",
        "## external memory lookup\n",
        "def memory_lookup(embeddings, query_value, column_values, lookup_map, column_map, cond_col, threshold=1.0):\n",
        "    lookup_value = None\n",
        "    query_value = query_value.replace('`','').strip()\n",
        "    sorted_sim, index = compute_cosine(query_value, embeddings)\n",
        "    if sorted_sim >= .70:\n",
        "        lookup_value = column_values[index]\n",
        "    else:\n",
        "        for col in list(lookup_map.keys()):\n",
        "            embeddings = lookup_map[col]\n",
        "            sorted_sim, index = compute_cosine(query_value, embeddings)\n",
        "            if sorted_sim >= .95:\n",
        "                lookup_value = column_map[col].vlaues[index]\n",
        "                cond_col = col\n",
        "            break\n",
        "    return (cond_col, lookup_value)\n",
        "\n",
        "## compute cosine similarity between matrix of candidattes and a query vector      \n",
        "def compute_cosine(query_value, embeddings):\n",
        "    query_embedding = get_embedding(query_value).to(torch_device)\n",
        "    embeddings = embeddings.to(torch_device)\n",
        "    \n",
        "    sim = cos_sim(embeddings, query_embedding)\n",
        "    sorted_sim, indices = torch.sort(sim, axis=0, descending=True)\n",
        "    return sorted_sim[0][0].item(), indices[0][0].item()\n",
        "\n",
        "## define sql augment function to resolved the ambigous entities\n",
        "sql_operators = ['>', '=', '<', '>=', '<=', '<>']\n",
        "agg_operators = ['MAX', 'AVG', 'MIN', 'COUNT', 'SUM']\n",
        "def augment_sql(sql, header, rows, header_types, question, lookup_value=False):\n",
        "    header = header\n",
        "    rows = rows\n",
        "    header_types = header_types\n",
        "    encoded_data, table = encode_data(rows, header, header_types)\n",
        "    \n",
        "    try:\n",
        "        select_clause = sql.split('FROM')[0].strip().split('SELECT')[1]\n",
        "        agg_clause = [agg_operator for agg_operator in agg_operators if agg_operator in select_clause]\n",
        "        select_cols = [column for idx,column in enumerate(header) if f\"<col{idx}>\" in select_clause]\n",
        "        where_clause = []\n",
        "        where_conditions = []\n",
        "        if 'WHERE' in sql:\n",
        "            where_clause = sql.split('WHERE')[1].split('AND')\n",
        "            for condition in where_clause:\n",
        "                column = [(column, f\"<col{idx}>\") for idx,column in enumerate(header) if f\"<col{idx}>\" in condition]\n",
        "                operator = None\n",
        "                if len(column):\n",
        "                    condition = condition.replace(column[0][1],column[0][0])\n",
        "                    for op in sql_operators:\n",
        "                        if op in condition:\n",
        "                            operator = op\n",
        "                else:\n",
        "                    break\n",
        "               \n",
        "                if len(column) and operator:\n",
        "                    cond_col, operator, con_val = column[0][0], operator, condition.split(operator)[1]\n",
        "                    cond_col_type = header_types[header.index(cond_col)]\n",
        "                    if operator == '=' and cond_col_type == 'text':\n",
        "                        if not lookup_value:\n",
        "                            cond_col, lookup_value = memory_lookup(\n",
        "                                embeddings=encoded_data[cond_col], \n",
        "                                query_value=con_val, \n",
        "                                column_values=table[cond_col].values ,\n",
        "                                lookup_map= encoded_data,\n",
        "                                column_map=table,\n",
        "                                cond_col=cond_col,\n",
        "                                threshold=1.0\n",
        "                            )\n",
        "                        else: \n",
        "                            lookup_value = con_val.replace('`','').strip()\n",
        "                        if lookup_value:\n",
        "                            where_conditions.append(f\"{cond_col} {operator} \\'{lookup_value}\\'\")\n",
        "                    elif operator in ['>', '<', '>=', '<=', '<>'] and cond_col_type == 'real':\n",
        "                        con_val = con_val.replace('`','').strip()\n",
        "                        not_number = True\n",
        "                        for x in con_val:\n",
        "                            if x.isdigit() or x == '.':\n",
        "                                continue\n",
        "                            else:\n",
        "                                not_number = False\n",
        "                        if not_number and con_val in question:\n",
        "                            where_conditions.append(f\"{cond_col} {operator} \\'{con_val}\\'\")\n",
        "                        \n",
        "            \n",
        "    except Exception as e:\n",
        "        #print('error parsing sql', e)\n",
        "        return None, None\n",
        "    where_final = \" AND \".join(where_conditions)\n",
        "    agg_final = \"\"\n",
        "    if len(agg_clause):\n",
        "        agg_final = agg_clause[0]\n",
        "    select_final = f\"SELECT {agg_final} \"+ \" , \".join(select_cols)\n",
        "    table_name = str(uuid.uuid4())\n",
        "    sql_final = f\"{select_final} FROM table \"\n",
        "    if len(where_conditions):\n",
        "        sql_final += f\"WHERE {where_final} \"\n",
        "    return (sql_final, table_name, agg_clause, select_cols, where_conditions)\n",
        "  \n",
        "final_sql, _, _, _, _ = augment_sql(prediction, header, rows, header_column_types, question=question_schema, lookup_value=False)\n",
        "print(final_sql)\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SELECT  Nationality FROM table WHERE Player = 'Terrence Ross' \n"
          ]
        }
      ]
    }
  ]
}